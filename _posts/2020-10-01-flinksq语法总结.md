> [https://support.huaweicloud.com/sqlref-flink-dli/dli_08_0207.html](https://support.huaweicloud.com/sqlref-flink-dli/dli_08_0207.html)

#### 复杂数据类型

##### MAP

##### ARRAY

##### ROW

```
Row 一组命名的字段,字段类型可以不同

如: ROW<a1 TYPE1, a2 TYPE2>
构造方式 Row('1',2) as v1
应用方式 变量名.字段名   例如 v1.a1


create source t_name (
	id string,
  type sttring,
  addr Row(province string, city string, state string),
  person Row(age int, sex boolean, name string)
)


```

#### over 窗口

```
select [column_list]
 FROM
   (
     select [column_list],
            row_number() over ([Partition By col1[,clo2]] order by time_attr_col [asc|desc]) as rownum
     from table_name
   ) where row_num = 1;


 row_number() 给分区内的每行数据分配一个唯一的序号(从1开始)
 partition by col1[,col2...] 指定分区列, 即去重的keys
 order by time_attr_col [asc|desc]
 		指定排序列。时间属性目前仅支持proctime, 暂不支持rowtime.
    	按asc排序保留第一条
    	按desc排序保留最后一条


1) 需要两层Query
		a) 内层查询使用row_number() over 窗口函数对分区内(通过partition by指定)的数据根据排序列
    	 (通过order by 指定)标上排名(rownum)
    b) 外层查询对排名取第一个
2) 外层查询where条件中,必须通过如row_num=1指定, flink才能将其识别为去重操作。
3) 会产生Retraction
4）仅支持Blink planner
```

##### over 使用示例

```
SELECT userID, eventType, eventTime, productID
   FROM (
      SELECT *,
       ROW_NUMBER() OVER (PARTITION BY userID, eventType, eventTime ORDER BY proctime DESC) AS rownum
       FROM source_kafka
    ) t WHERE rownum = 1;

// Kafka中逐条输入如下数据
{"userID":"user_1","eventType":"click","eventTime":"2015-01-01 01:00:00","productID":"product_1"}
{"userID":"user_1","eventType":"click","eventTime":"2015-01-01 01:00:00","productID":"product_2"}
{"userID":"user_1","eventType":"click","eventTime":"2015-01-01 01:00:00","productID":"product_3"}

// 输出
(true,user_1,click,2015-01-01 01:00:00,product_1)
(false,user_1,click,2015-01-01 01:00:00,product_1)
(true,user_1,click,2015-01-01 01:00:00,product_2)
(false,user_1,click,2015-01-01 01:00:00,product_2)
(true,user_1,click,2015-01-01 01:00:00,product_3)


SELECT userID, eventType, eventTime, productID
   FROM (
      SELECT *,
       ROW_NUMBER() OVER (PARTITION BY userID, eventType, eventTime ORDER BY proctime ASC) AS rownum
       FROM source_kafka
    ) t WHERE rownum = 1;

// Kafka中逐条输入如下数据
{"userID":"user_1","eventType":"click","eventTime":"2015-01-01 01:00:00","productID":"product_1"}
{"userID":"user_1","eventType":"click","eventTime":"2015-01-01 01:00:00","productID":"product_2"}
{"userID":"user_1","eventType":"click","eventTime":"2015-01-01 01:00:00","productID":"product_3"}

// 输出 只输出一条
(true,user_1,click,2015-01-01 01:00:00,product_1)


```

##### 底层原理

```

deduplication 算子
  depulication算子为每一个partition key维护了一个value state, 用于去重。
  每来一条数据时从当前partition key的value state取获取value, 如果不为空, 说明已经有数据来过了,
  当前这一条数据就是重复数据, 就不往下游算子下发了, 如果为空, 说明之前有数据来过, 当前这一条数据就是
  就是第一条数据, 则把当前的value state值设置为ture, 往下游算子下发数据。
```

[https://www.jianshu.com/p/8696e4fee6e7](https://www.jianshu.com/p/8696e4fee6e7)

###

### sql 语法

#### if 语句

```
T if( boolean testcondition, T valueTure, T valueFalseOrNull)
	T 代表任意类型的返回值

select if(int1 < int2, str1,str2) as result from T1

```

#### last_value

```
聚合函数 last_value
flink 中使用last_value 函数返回指定数据流的最后1条非null 数据

t last_value(T value)
t last_value(T value, bigint order)

入参:
	value 任意参数类型
  order bigint
获取数据流的最后1条非null数据。根据order 判定last_value锁在的行, 取order值最大的记录最为last_value。
```

#### ldap&rdap

```
select LPAD(frname, 7 ,'xo') as '左填充'
			 RPAD(frname, 8 ,'xo') as '右填充'
  from ent_file where id = 11449

select LPAD(name,7,'xo') as ‘左填充’, RPAD(name,8,'xo') as '右填充' from end_file where id = 11449;
| 左填充  |  右填充    |
|xoxossg |  ssgxoxox |
```

#### cast

```
cast('0098' as int) 输出 98
```

### 其他

```
RFC3339 时间标识  "2020-11-08T08:18:46+08:00"
```
