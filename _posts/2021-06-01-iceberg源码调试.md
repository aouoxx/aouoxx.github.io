源码编译

```java
避免过多的checker信息
    将文件checkstyle.xml中的
    <module name="Checker">
      //....删除掉
    </module>

修改gradle的maven信息
	buildscript {
          repositories {
            maven{ url 'https://mirrors.huaweicloud.com/repository/maven/' }
            gradlePluginPortal()
          }
          ...
     }
     allprojects {
          group = "org.apache.iceberg"
          version = getProjectVersion()
          repositories {
            maven {
              url 'https://mirrors.huaweicloud.com/repository/maven/'
            }
            mavenCentral()
            mavenLocal()
          }
      }
修改gradle.properties修改编译flink的版本
	systemProp.defaultFlinkVersions=1.13

 ./gradlew build -x test -x integrationTest
```

```java
iceberg流读编译

HiveConf hiveConf = new HiveConf();
String icebergNamespace = "icebergNamespace";
String icebergTableName = "icebergTableName";
TableIdentifier identifier = TableIdentifier.of(Namespace.of(icebergNamespace), icebergTableName);
HashMap<String, String> properties = new HashMap<>();
String icebergUri = "icebergUri";
properties.put("uri", icebergUri);
CatalogLoader catalogLoader = CatalogLoader.hive("hive", hiveConf, properties);
TableLoader tableLoader = TableLoader.fromCatalog(catalogLoader, identifier);

StreamExecutionEnvironment environment = StreamExecutionEnvironment.createLocalEnvironment();

DataStream<RowData> rowStream = FlinkSource.forRowData()
    .env(environment)
    .tableLoader(tableLoader)
    .streaming(true)
    .build();
```

```java
org.apache.iceberg.flink.source.FlinkSource.Builder
	构建FlinkSource

public static class FlinkSource.Builder {
    // todo flink stream 环境
    private StreamExecutionEnvironment env;
    // todo iceberg table
    private Table table;
    // todo 用于load iceberg表
    private TableLoader tableLoader;
    // todo iceberg 的schema描述
    private TableSchema projectedSchema;
    // todo 进行icebeg 表读取的配置参数
    private ReadableConfig readableConfig = new Configuration();
    // todo 构建scan 上下文(很关键); 能够进行filter/limit/properties设置/snapshotId指定, 是否区分大小写
    // todo datafile  split/数据回放 等
    private final ScanContext.Builder contextBuilder = ScanContext.builder();
}
```

```java
引用相关 Iceberg 接口和方法：

BaseTransaction.commitCreateTransaction()执行 tableOperation 事务
TableMetadataParser.write()写入元数据 json 文件
```

![image.png](https://cdn.nlark.com/yuque/0/2021/png/659846/1640095680496-ba273814-406f-493e-8839-70a51e83c594.png#clientId=u4a4d9a8f-259b-4&from=paste&height=189&id=ufda6da43&margin=%5Bobject%20Object%5D&name=image.png&originHeight=269&originWidth=511&originalType=binary&ratio=1&size=20678&status=done&style=none&taskId=ubc4a4e41-7bf7-415b-b7b0-b2a66df7fc6&width=359.5)

```java
在有多个IcebergStreamWriter和一个IcebergFileCommitter的情况下,上游的数据写到IcebergStreamWriter的时候,
每个Writer做的时间都是去写dataFiles文件。

当每个Writer写完自己当前这一批datafiles小文件的时候,就会发送消息给IcebergFileCommitter,告诉它可以提交了。
而IcebergFileCommitter收到信息的时候,就一次性将datafiles的文件提交,进行一次commit操作。

commit操作本身只是对一个原始信息的修改,当数据都已经写到磁盘了,只是让其从不可见变成可见。这种情况下,Iceberg只需要用一个
commit即可完成数据从不可见变成可见的过程。

IcebergStreamWriter的设计比较简单, 主要任务是把记录转换成DataFile, 并没有复杂的state需要设计。

IcebergFilesCommitter相对复杂一点, 它为每个checkpointId维护了一个DataFile文件列表, 这样即使中间有某个checkpoint的transaction
提交失败了, 它的Data
```

![image.png](https://cdn.nlark.com/yuque/0/2021/png/659846/1640096926733-c1956ead-fa53-4080-b301-f381e89286c6.png#clientId=uc0bda46d-36d6-4&from=paste&height=240&id=ub8813510&margin=%5Bobject%20Object%5D&name=image.png&originHeight=480&originWidth=1322&originalType=binary&ratio=1&size=306626&status=done&style=none&taskId=uda371113-5688-45c7-9ca2-cd4f6db3df0&width=661)

```java
首先，如果有一个 write 操作，在写 snapsho-1 的时候，snapshot-1 是虚线框，也就是说此时还没有发生 commit 操作。
这时候对 snapshot-1 的读其实是不可读的，因为用户的读只能读到已经 commit 之后的 snapshot。
发生 commit 之后才可以读。同理，会有 snapshot-2，snapshot-3。

Iceberg 提供的一个重要能力, 就是读写分离能力。
 在对 snapshot-4 进行写的时候，其实是完全不影响对 snapshot-2 和 snapshot-3 的读。
 Iceberg 的这个能力对于构建实时数仓是非常重要的能力之一。

同理，读也是可以并发的，可以同时读 s1、s2、s3 的快照数据，这就提供了回溯读到 snapshot-2 或者 snapshot-3 数据的能力。
Snapshot-4 写完成之后，会发生一次 commit 操作，这个时候 snapshot-4 变成了实心，此时就可以读了。
另外，可以看到 current Snapshot 的指针移到 s4，也就是说默认情况下，
用户对一张表的读操作，都是读 current Snapshot 指针所指向的 Snapshot，但不会影响前面的 snapshot 的读操作。
```

mvcc&乐观并发

```java
一个表元数据文件与另一个表元数据文件的原子交换提供了可序列化的隔离。
读取器在加载表元数据时使用当前的快照,在刷新并拾取新的元数据位置之前不受更改的影响。


Flink iceberg sink的设计原理由iceberg采用乐观锁的方式来实现transaction的提交,即也就是说两个线程/进程同时提交更改事务到iceberg时
后开始的一方会不断重试, 等先开始的一方顺利提交之后在重新读取metadata信息提交transaction.
由于该特点,采用多个算子去提交transaction是不合适的,容易造成大量事务冲突, 导致重试。
```

### catalog

```java
org.apache.iceberg.hive.HiveCatalog#fileIO
 org.apache.iceberg.CatalogUtil#loadFileIO (自定义,配置参数"io-impl")
 org.apache.iceberg.hadoop.HadoopFileIO (构建类信息,HadoopFileIO)
   org.apache.iceberg.hadoop.HadoopFileIO#newOutputFile
     org.apache.iceberg.hadoop.HadoopOutputFile#fromPath
      org.apache.iceberg.hadoop.Util#getFs
        org.apache.hadoop.fs.Path#getFileSystem (获取FileSystem)
      org.apache.iceberg.hadoop.HadoopOutputFile#fromPath (获取hadoopOutputFile)

public class HadoopOutputFile implements OutputFile {
  private final FileSystem fs;
  private final Path path;
  private final Configuration conf;
  // ...省略非核心代码
  public static OutputFile fromPath(Path path, Configuration conf) {
    FileSystem fs = Util.getFs(path, conf);
    return fromPath(path, fs, conf);
  }

  public static OutputFile fromPath(Path path, FileSystem fs, Configuration conf) {
    return new HadoopOutputFile(fs, path, conf);
  }

  private HadoopOutputFile(FileSystem fs, Path path, Configuration conf) {
    this.fs = fs;
    this.path = path;
    this.conf = conf;
  }

  @Override
  public PositionOutputStream create() {
    return HadoopStreams.wrap(fs.create(path, false /* createOrOverwrite */));
  }
  // 可以进行文件内容的覆写
  @Override
  public PositionOutputStream createOrOverwrite() {
     return HadoopStreams.wrap(fs.create(path, true /* createOrOverwrite */));
  }

  ...
}
```

```java


修改hive的版本 (versions.props):
 org.apache.hive:* = 2.3.9
```

#### iceberg 建表

```java
/**
     * 创建表信息
     */
public void buildTable(){
    //   表的schema信息
    Types.StructType structType = Types.StructType.of(
        required(0, "name", Types.LongType.get(), "order_id"),
        required(1, "order_date", Types.DateType.get()),
        required(2, "account_number", Types.LongType.get()),
        required(3, "customer", Types.StringType.get()),
        optional(4, "country", Types.StringType.get(), "customer country")
    );
    AtomicInteger nextFieldId = new AtomicInteger(1);
    Type icebergSchema = TypeUtil.assignFreshIds(structType, nextFieldId::getAndIncrement);
    Schema schema =   new Schema(icebergSchema.asStructType().fields());
    // 分区信息
    List<String> partitionFields = new ArrayList<>();
    partitionFields.add("order_date");
    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);
    for (String field : partitionFields) {
        parsePartitionField(builder, field);
    }
    PartitionSpec partitionSpec =  builder.build();
    // 构建数据湖表名称
    TableIdentifier tableIdent = TableIdentifier.of(Namespace.of(DBNAME), "user_msg_iceberg_2x");
    // 构建表信息
    Map<String,String> prop = new HashMap();
    prop.put("iceberg.name","ssgao");
    prop.put("iceberg.addr","hangzhou");
    prop.put("iceberg.sch","jiangnangguojicheng");
    Transaction txn = catalog.buildTable(tableIdent, schema)
        .withPartitionSpec(partitionSpec)
        .withProperties(prop)
        .createTransaction();
    txn.commitTransaction();
}
```

```java
org.apache.iceberg.Transactions#createTableTransaction
  org.apache.iceberg.BaseMetastoreCatalog#newTableOps -- 构建 HiveTableOperations, 见下面[HiveTableOptions]
  org.apache.iceberg.TableMetadata#newTableMetadata(..)
    org.apache.iceberg.TableMetadata#newTableMetadata(..) -- 构建 TableMetadata 见下面 [TableMetadata]
  org.apache.iceberg.Transactions#createTableTransaction

 --  (new BaseTransaction(tableName, ops, TransactionType.CREATE_TABLE, start);

```

##### HiveTableOperations

```java
public class HiveTableOperations{


    protected HiveTableOperations(Configuration conf, ClientPool metaClients, FileIO fileIO, String catalogName, String database, String table) {
        this.conf = conf;
        this.metaClients = metaClients;
        this.fileIO = fileIO;
        this.fullName = catalogName + "." + database + "." + table;
        this.database = database;
        this.tableName = table;
        this.lockAcquireTimeout =
            conf.getLong(HIVE_ACQUIRE_LOCK_TIMEOUT_MS, HIVE_ACQUIRE_LOCK_TIMEOUT_MS_DEFAULT);
        this.lockCheckMinWaitTime =
            conf.getLong(HIVE_LOCK_CHECK_MIN_WAIT_MS, HIVE_LOCK_CHECK_MIN_WAIT_MS_DEFAULT);
        this.lockCheckMaxWaitTime =
            conf.getLong(HIVE_LOCK_CHECK_MAX_WAIT_MS, HIVE_LOCK_CHECK_MAX_WAIT_MS_DEFAULT);
        this.metadataRefreshMaxRetries =
            conf.getInt(HIVE_ICEBERG_METADATA_REFRESH_MAX_RETRIES, HIVE_ICEBERG_METADATA_REFRESH_MAX_RETRIES_DEFAULT);
        long tableLevelLockCacheEvictionTimeout =
            conf.getLong(HIVE_TABLE_LEVEL_LOCK_EVICT_MS, HIVE_TABLE_LEVEL_LOCK_EVICT_MS_DEFAULT);
        initTableLevelLockCache(tableLevelLockCacheEvictionTimeout);
      }

}

```

##### TableMetadata

_**iceberg 的表元数据**_

```java
public class TableMetadata implements Serializable {
  static final long INITIAL_SEQUENCE_NUMBER = 0;
  static final long INVALID_SEQUENCE_NUMBER = -1;
  static final int DEFAULT_TABLE_FORMAT_VERSION = 1;
  static final int SUPPORTED_TABLE_FORMAT_VERSION = 2;
  static final int INITIAL_SPEC_ID = 0;
  static final int INITIAL_SORT_ORDER_ID = 1;
  static final int INITIAL_SCHEMA_ID = 0;

  TableMetadata(String metadataFileLocation,
                int formatVersion,
                String uuid,
                String location,
                long lastSequenceNumber,
                long lastUpdatedMillis,
                int lastColumnId,
                int currentSchemaId,
                List<Schema> schemas,
                int defaultSpecId,
                List<PartitionSpec> specs,
                int lastAssignedPartitionId,
                int defaultSortOrderId,
                List<SortOrder> sortOrders,
                Map<String, String> properties,
                long currentSnapshotId,
                List<Snapshot> snapshots,
                List<HistoryEntry> snapshotLog,
                List<MetadataLogEntry> previousFiles,
                List<MetadataUpdate> changes) {
    Preconditions.checkArgument(specs != null && !specs.isEmpty(), "Partition specs cannot be null or empty");
    Preconditions.checkArgument(sortOrders != null && !sortOrders.isEmpty(), "Sort orders cannot be null or empty");
    Preconditions.checkArgument(formatVersion <= SUPPORTED_TABLE_FORMAT_VERSION, "Unsupported format version: v%s", formatVersion);
    Preconditions.checkArgument(formatVersion == 1 || uuid != null, "UUID is required in format v%s", formatVersion);
    Preconditions.checkArgument(formatVersion > 1 || lastSequenceNumber == 0, "Sequence number must be 0 in v1: %s", lastSequenceNumber);
    Preconditions.checkArgument(metadataFileLocation == null || changes.isEmpty(), "Cannot create TableMetadata with a metadata location and changes");

    this.metadataFileLocation = metadataFileLocation;
    this.formatVersion = formatVersion;
    this.uuid = uuid;
    this.location = location;
    this.lastSequenceNumber = lastSequenceNumber;
    this.lastUpdatedMillis = lastUpdatedMillis;
    this.lastColumnId = lastColumnId;
    this.currentSchemaId = currentSchemaId;
    this.schemas = schemas;
    this.specs = specs;
    this.defaultSpecId = defaultSpecId;
    this.lastAssignedPartitionId = lastAssignedPartitionId;
    this.defaultSortOrderId = defaultSortOrderId;
    this.sortOrders = sortOrders;
    this.properties = properties;
    this.currentSnapshotId = currentSnapshotId;
    this.snapshots = snapshots;
    this.snapshotLog = snapshotLog;
    this.previousFiles = previousFiles;

    // changes are carried through until metadata is read from a file
    this.changes = changes;

    this.snapshotsById = indexAndValidateSnapshots(snapshots, lastSequenceNumber);
    this.schemasById = indexSchemas();
    this.specsById = indexSpecs(specs);
    this.sortOrdersById = indexSortOrders(sortOrders);

    HistoryEntry last = null;
    for (HistoryEntry logEntry : snapshotLog) {
      if (last != null) {
        Preconditions.checkArgument(
            (logEntry.timestampMillis() - last.timestampMillis()) >= -ONE_MINUTE,"[BUG] Expected sorted snapshot log entries.");
      }
      last = logEntry;
    }
    if (last != null) {
      Preconditions.checkArgument(
          // commits can happen concurrently from different machines.
          // A tolerance helps us avoid failure for small clock skew
          lastUpdatedMillis - last.timestampMillis() >= -ONE_MINUTE,
          "Invalid update timestamp %s: before last snapshot log entry at %s",
          lastUpdatedMillis, last.timestampMillis());
    }

    MetadataLogEntry previous = null;
    for (MetadataLogEntry metadataEntry : previousFiles) {
      if (previous != null) {
        Preconditions.checkArgument(
            // commits can happen concurrently from different machines.
            // A tolerance helps us avoid failure for small clock skew
            (metadataEntry.timestampMillis() - previous.timestampMillis()) >= -ONE_MINUTE,
            "[BUG] Expected sorted previous metadata log entries.");
      }
      previous = metadataEntry;
    }
    // Make sure that this update's lastUpdatedMillis is > max(previousFile's timestamp)
    if (previous != null) {
      Preconditions.checkArgument(
          // commits can happen concurrently from different machines.
          // A tolerance helps us avoid failure for small clock skew
          lastUpdatedMillis - previous.timestampMillis >= -ONE_MINUTE,
          "Invalid update timestamp %s: before the latest metadata log entry timestamp %s",
          lastUpdatedMillis, previous.timestampMillis);
    }

    Preconditions.checkArgument(
        currentSnapshotId < 0 || snapshotsById.containsKey(currentSnapshotId),
        "Invalid table metadata: Cannot find current version");
  }

}
```
